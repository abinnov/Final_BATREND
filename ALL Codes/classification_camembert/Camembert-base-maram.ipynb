{"cells":[{"cell_type":"markdown","metadata":{"id":"UlzoB2Tw5xb4"},"source":["**Requirments**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43848,"status":"ok","timestamp":1653901861479,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"D7PV_t7IyrC5","outputId":"c57aa318-e4d0-4d1c-dad0-08ba052686be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 5.4 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 40.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 30.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emoji\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 6.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=f3dd068135f33eaa3253ea839c7b9c94749fd42bd4640478a09a90d14b099b34\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.7.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidecode\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 5.6 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.4\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install emoji\n","!pip install unidecode\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OV7Yz0ypCIvl"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyH6ymlN4prg"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import torch\n","import os\n","from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm \n","import torch.nn.functional as F\n"]},{"cell_type":"markdown","metadata":{"id":"n4Om6Y0v9ITU"},"source":["**config**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEOSZChM8Sj2"},"outputs":[],"source":["EPOCHS = 4\n","#DATA_PATH =\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/class_data1.csv\"\n","model_name='camembert-base'"]},{"cell_type":"markdown","metadata":{"id":"lUiX7lkx5g06"},"source":["**Utils**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vp_8Laft5jmV"},"outputs":[],"source":["def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  acc = accuracy_score(labels, preds)\n","  return {'accuracy': acc,}"]},{"cell_type":"markdown","metadata":{"id":"IZRIi4_6w9FL"},"source":["**Cleaning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-9_6qMmx04F"},"outputs":[],"source":["import random\n","import pandas as pd\n","env_label=['écologie', 'énergie', 'climat', 'pollution', 'environnement', 'gaz', 'nature', \"recyclage\", 'déforestation', 'urbanisation', 'ressources naturelles', 'biologique', 'poubelle', 'nucléaire',  'forage',  'pesticide', 'renouvelable',  'éboueur', 'déchet', 'ordures', 'contamination']\n","\n","imm_label=['migration', 'immigration', 'asile', 'réfugié', 'immigré', 'étranger', 'clandestin', 'visa', 'émigration', 'nationalité', 'colon', 'exilé', 'migratoire', 'frontières', 'exode', 'transhumance', 'délocalisation', 'déménager', 'déplacement', 'invasion', 'fuite', 'escapade', 'exil', 'exode', 'nomade', 'voyageur', 'fuite des cerveaux', 'intégration', 'regroupement familial', 'fuyant']\n","\n","eco_label= [\"pouvoir achat\", 'taxe','coût' ,'inflation', 'consommation', 'prix',\"fiscalité\",'salaire','revenus', 'TVA', 'impôts', 'retraite','smic','argent', 'crédit','économie','épargne','loyers','dette','Taux d’intérêt' , 'nettes', 'marchandises' ,\"financer\", \"augmentation des salaires\" , \"richesse\", \"rémunération\",\"contrat\",\"achat\",\"finance\", 'vente','marché', 'cotisation']\n","\n","sante_label=['santé', 'virus', 'maladie', 'médecine', 'médecin','vaccin', 'covid', 'masque', 'hygiène', 'bactérie', 'médicament', 'hôpital',  'épidémie', 'assurance', 'prévention sanitaire', 'remède', 'malade', 'patient', 'thérapie', 'handicap', 'clinique', 'pharmacie', 'pharmacien', 'guérison', 'soin', 'vacciné', 'infirmier']\n","\n","sec_label=['sécurité', 'insécurité', 'agression', 'défense', 'délinquance', 'terrorisme', 'police', 'violence', 'criminalité', 'paix', 'surveillance', 'protection', 'guerre', 'soldat', 'menace', 'danger', 'piratage', 'cybersécurité', 'attaque', 'gendarmerie', 'armée']\n","\n","ret_label=['retraite', 'préretraite', 'pension de retraite', 'fonds de pension', \"caisses d'épargne\" , 'les vieux', 'assurance-retraite', '65 ans', '60 ans' , 'âge de départ', 'retraité', 'vieillesse']\n","\n","egal_label=['inégalité', 'racisme', 'discrimination', 'justice', 'équité', 'égalitaire', 'hiérarchie', 'démocratique', 'caste', 'sexisme', 'préjugé', 'préférence', 'persécution', 'déséquilibre', 'disparité', 'disproportion', 'dissemblance', 'ségrégation', 'inégalitarisme', 'droits', 'misogynie', 'fanatisme', 'chauvinisme', 'xénophobie', 'incitation', 'humiliation', 'insulte', 'intolérance', 'supériorité', 'esclavage']\n","\n","laicite_label=['juif','chritien','laïcité', 'laïque', 'religion d’état', 'religieux', 'laïcisation', 'sécularité', 'séparation', 'islamophobie', 'religion','signe religieux', 'islamofascistes', 'voile' , 'séparatisme religieux' , 'loi de 1905' , 'musulman']\n","df = pd.DataFrame(columns = ['text', 'label'])\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(eco_label, k=20)), 'label': 0}, ignore_index=True)\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(env_label, k=20)), 'label': 1}, ignore_index=True)\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(imm_label, k=20)), 'label': 2}, ignore_index=True)\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(egal_label, k=20)), 'label': 3}, ignore_index=True)\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(laicite_label, k=20)), 'label': 4}, ignore_index=True)\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(ret_label, k=20)), 'label': 5}, ignore_index=True)\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(sante_label, k=20)), 'label': 6}, ignore_index=True)\n","for i in range(500):\n","  df = df.append({'text': \" \".join(random.choices(sec_label, k=20)), 'label': 7}, ignore_index=True)\n","df.to_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/mots_cle500.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvptH4UqxAKz"},"outputs":[],"source":["from emoji import UNICODE_EMOJI\n","import unidecode\n","import re\n","\n","class clean_data():\n","    # remove URLs   \n","    def remove_URL(self,text):\n","        url = re.compile(r'https?://\\S+|www\\.\\S+')\n","        return url.sub(r'',text)\n","\n","    # remove htmls\n","    def remove_html(self,text):\n","        html=re.compile(r'<.*?>')\n","        return html.sub(r'',text)\n","    \n","    # remove punct\n","    def remove_punct(self,text):\n","        punc = '''!()[]{};:\"\\,<>./?@$%^&#*_~'''\n","        for ele in text:\n","            if ele in punc:\n","                text= text.replace(ele, \"\")\n","        return text\n","    # remove other ...\n","    def remove_other (self,text) : \n","        text = text.rstrip()\n","        text = re.sub(' +', ' ', text)\n","        text = re.sub('-', ' ', text)\n","        text = re.sub('\\[.*?\\]', '', text)\n","        text = re.sub('<.*?>+', '', text)\n","        text = re.sub('\\n', '', text)\n","        text = re.sub('\\w*\\d\\w*', '', text)\n","        return(text)\n","\n","    # lower   \n","    def lower_text(self, text):\n","        return text.lower()\n","\n","    # remove emojis\n","    def remove_emoji(self, text):\n","        to_replace = UNICODE_EMOJI[\"en\"]\n","        result = text\n","        for x in to_replace:\n","            result = result.replace(x, \"\")\n","        return result\n","\n","    # remove accents\n","    def remove_accents(self, text):\n","        return unidecode.unidecode(text)\n","\n","    # clean text\n","    def clean(self,text) : \n","        text = self.lower_text(text)\n","        text = self.remove_URL(text)\n","        text = self.remove_html(text)\n","        text = self.remove_punct(text)\n","        text = self.remove_emoji(text)\n","        text = self.remove_other (text)\n","        return text\n"]},{"cell_type":"markdown","metadata":{"id":"OdEE_I8o72cW"},"source":["**Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3anzGtoe7wsN"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset , DataLoader\n","class Dataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings= encodings\n","        self.labels= labels\n","    \n","    def __getitem__(self, idx):\n","        item={key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels']=torch.tensor(self.labels[idx])\n","        return item\n","    def __len__(self):\n","        return len(self.labels)\n"]},{"cell_type":"markdown","metadata":{"id":"gqZ70gkR8J_B"},"source":["**Fine Tunning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAtK8v_B8M9B"},"outputs":[],"source":["\n","def fine_tuning(train_texts,test_texts,train_label,test_label):\n","  print('Step 1 : Loading Model & Tokenizer ')\n","  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  print('Step 2 : Encoding Tokens')\n","  train_encodings= tokenizer(train_texts,truncation=True,padding=True,max_length = 30)\n","  test_encoding=tokenizer(test_texts,truncation=True,padding=True,max_length = 30)\n","  print ('Step 3 : Creating Dataset')\n","  traindt = Dataset(train_encodings,train_label)\n","  testdt=Dataset(test_encoding,test_label)  \n","  training_args = TrainingArguments(\n","      output_dir=\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/log/\",\n","      num_train_epochs=1,\n","      per_device_train_batch_size=64,\n","      per_device_eval_batch_size=64,\n","      warmup_steps=500,\n","      weight_decay=0.01,\n","      evaluation_strategy='epoch',\n","      logging_dir='/content/drive/MyDrive/Colab_Notebooks/classification_camembert/log/',\n","      learning_rate= 0.00005,\n","      logging_steps=80000,\n","      save_strategy=\"no\"\n","  )\n","  print ('Step 4 : Training ')\n","  for i in tqdm(range(1,6)) : \n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        compute_metrics=compute_metrics,\n","        train_dataset=traindt,\n","        eval_dataset=testdt\n","        )\n","    trainer.train()\n","    PATH_TO_SAVE = f\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_{i}\"\n","    model.save_pretrained(PATH_TO_SAVE)\n","    model = AutoModelForSequenceClassification.from_pretrained(PATH_TO_SAVE)\n","  return traindt , testdt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22043,"status":"ok","timestamp":1653905169638,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"h7cNudrR_Z8W","outputId":"35839028-7c25-4c82-c4b8-3e37518690d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                    text label\n","0      Alors que le projet de loi est examiné lundi à...     0\n","1      Le 1er juin 2009, le vol Air France AF 447 dis...     0\n","2      Le projet de loi, qui vise à donner un nouveau...     0\n","3      L’abondance des récoltes a fait baisser le cou...     0\n","4      La question de l’avenir de l’Etat-providence d...     0\n","...                                                  ...   ...\n","15956  Développé par le studio Westwood à la fin de l...     6\n","15957  Lors d’une conférence, Fang Binxing a tenté, e...     6\n","15958  Rentrée littéraire. Chaud New York ! Avec l'éc...     6\n","15959  Sous la direction de Maurice Godelier, les Edi...     6\n","15960  Un doublé de son attaquant vedette et une erre...     6\n","\n","[15961 rows x 2 columns]\n","                                                    text label\n","0      alors que le projet de loi est examiné lundi à...     0\n","1      le  juin  le vol air france af  disparaissait ...     0\n","2      le projet de loi qui vise à donner un nouveau ...     0\n","3      l’abondance des récoltes a fait baisser le cou...     0\n","4      la question de l’avenir de l’etat providence d...     0\n","...                                                  ...   ...\n","15956  développé par le studio westwood à la fin de l...     6\n","15957  lors d’une conférence fang binxing a tenté en ...     6\n","15958  rentrée littéraire chaud new york avec l'écriv...     6\n","15959  sous la direction de maurice godelier les edit...     6\n","15960  un doublé de son attaquant vedette et une erre...     6\n","\n","[15961 rows x 2 columns]\n"]}],"source":["\"\"\"\n","#\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/train_cles.csv\"\n","train= pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n","train.dropna(subset=['text'],inplace=True)\n","X_train= (train.text.values.tolist())\n","y_train=train.label.values.tolist()\n","#train_texts,test_texts,train_label,test_label= train_test_split(comments,labels,shuffle=True,test_size=0.2)\n","test= pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n","test.dropna(subset=['text'],inplace=True)\n","X_test= (test.text.values.tolist())\n","y_test=test.label.values.tolist()\"\"\"\n","\n","#data= pd.read_csv(\"/content/drive/MyDrive/Full_Data.csv\")\n","#data=data[data['label']==6]\n","cles=pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/mots_cle500.csv\")\n","\"\"\"\n","cles=cles[cles['label']!=5]\n","cles=cles[cles['label']!=3]\n","di={0:0, 1:1, 2:2, 4:3,6:4,7:5,8:6}\n","cles['label']=cles['label'].map(di)\n","print(cles.label.tolist())\"\"\"\n","#eco=cles[cles['label']==0]\n","#cles=cles[cles['label']!=0]\n","#eco=pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/economie_cles.csv\")\n","#eco=eco.head(250)\n","non_cles=pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/keyWords_nonKW.csv\")\n","non_cles=non_cles[non_cles['label']!=5]\n","non_cles=non_cles[non_cles['label']!=3]\n","di={0:0, 1:1, 2:2, 4:3,6:4,7:5,8:6}\n","non_cles['label']=non_cles['label'].map(di)\n","\n","TOPICS  = ['economie','environnement','immigration_labeled',\n","           'laicite','sante','sécurité','autre']\n","           \n","Full_Data = pd.DataFrame([] ,  columns=['text','label'])\n","\n","for idx , topic in enumerate(TOPICS) : \n","  PATH = os.path.join('/content/drive/MyDrive/Topics/' ,topic +'.csv')\n","  df = pd.read_csv(PATH)[:1400] \n","  df['label'] = idx\n","  Full_Data = pd.concat([Full_Data,df],axis=0, ignore_index=False)\n","\n","pdList = [Full_Data, cles, non_cles]\n","df = pd.concat(pdList, ignore_index=True)\n","df.text=df.text.astype(str)\n","print(df)\n","for i in range(df.shape[0]):\n","  df.text[i]=clean_data().clean(df.text[i])\n","print(df)\n","X_train,X_test,y_train,y_test= train_test_split(df.text.tolist(),df.label.tolist(),shuffle=True,test_size=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yV3EmA0jEQAd","outputId":"37dbd064-e1d6-46c7-e27a-e8f91bc66170"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"camembert-base\",\n","  \"architectures\": [\n","    \"CamembertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Step 1 : Loading Model & Tokenizer \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading weights file https://huggingface.co/camembert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7e23f45751ad1fed420ca9f03bb37a279dc98a56c75bf25e671129237e2c893c.ee4d4253e08a7cf9697c0671fd8f022483dbf586691a7b32ead55493a34d72b2\n","Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"camembert-base\",\n","  \"architectures\": [\n","    \"CamembertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading file https://huggingface.co/camembert-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/dbcb433aefd8b1a136d029fe2205a5c58a6336f8d3ba20e6c010f4d962174f5f.160b145acd37d2b3fd7c3694afcf4c805c2da5fd4ed4c9e4a23985e3c52ee452\n","loading file https://huggingface.co/camembert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/84c442cc6020fc04ce266072af54b040f770850f629dd86c5951dbc23ac4c0dd.8fd2f10f70e05e6bf043e8a6947f6cdf9bb5dc937df6f9210a5c0ba8ee48e959\n","loading file https://huggingface.co/camembert-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/camembert-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/camembert-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"camembert-base\",\n","  \"architectures\": [\n","    \"CamembertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Step 2 : Encoding Tokens\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Step 3 : Creating Dataset\n","Step 4 : Training \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]***** Running training *****\n","  Num examples = 12768\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 200\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [200/200 1:05:42, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.821756</td>\n","      <td>0.893204</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 3193\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/config.json\n","Model weights saved in /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/pytorch_model.bin\n","loading configuration file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/config.json\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1\",\n","  \"architectures\": [\n","    \"CamembertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading weights file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/pytorch_model.bin\n","All model checkpoint weights were used when initializing CamembertForSequenceClassification.\n","\n","All the weights of CamembertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use CamembertForSequenceClassification for predictions without further training.\n"," 20%|██        | 1/5 [1:06:16<4:25:05, 3976.37s/it]***** Running training *****\n","  Num examples = 12768\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 200\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='153' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [153/200 48:01 < 14:56, 0.05 it/s, Epoch 0.76/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [200/200 1:08:22, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.433925</td>\n","      <td>0.920138</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3193\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_2/config.json\n","Model weights saved in /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_2/pytorch_model.bin\n","loading configuration file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_2/config.json\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_2\",\n","  \"architectures\": [\n","    \"CamembertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading weights file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_2/pytorch_model.bin\n","All model checkpoint weights were used when initializing CamembertForSequenceClassification.\n","\n","All the weights of CamembertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use CamembertForSequenceClassification for predictions without further training.\n"," 40%|████      | 2/5 [2:15:11<3:23:29, 4069.83s/it]***** Running training *****\n","  Num examples = 12768\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 200\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  5/200 00:57 < 1:02:25, 0.05 it/s, Epoch 0.02/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["fine_tuning(X_train,X_test,y_train,y_test)"]},{"cell_type":"markdown","metadata":{"id":"m2DLRHRUqR9F"},"source":["**Test 1 + 2:** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7HjEQTuqVHx"},"outputs":[],"source":["from tqdm import tqdm\n","def test_overfit_underfit (train_texts,test_texts,train_label,test_label,PATH) : \n","  model = AutoModelForSequenceClassification.from_pretrained(PATH)\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  # Test 1 : Underfit ( On Train Dataset ) : \n","  train_encoding=tokenizer(train_texts,truncation=True,padding=True,max_length = 20)\n","  train_dataset=Dataset(train_encoding,train_label)  \n","  train_loader=DataLoader(train_dataset, batch_size=64, shuffle=True)  \n","  total = 0 \n","  train_accuracy = 0  \n","  with torch.no_grad():\n","      for batch in (train_loader):\n","          input_ids=batch['input_ids']\n","          attention_mask=batch['attention_mask']\n","          labels=batch['labels']\n","          outputs=model(input_ids, attention_mask=attention_mask, labels=labels)\n","          predictions=F.softmax(outputs.logits, dim=1)\n","          pred= torch.argmax(predictions, dim=1)\n","          total += labels.size(0)\n","          train_accuracy += (pred == labels).sum().item()\n","      train_accuracy = (100 * train_accuracy / total)\n","\n","  print(\"train_accuracy:\",train_accuracy )\n","\n","  # Test 2 : Overfit  ( On Test Dataset ) : \n","  test_encoding=tokenizer(test_texts,truncation=True,padding=True,max_length = 20)\n","  test_dataset=Dataset(test_encoding,test_label)  \n","  test_loader=DataLoader(test_dataset, batch_size=64, shuffle=True)   \n","  total = 0 \n","  test_accuracy = 0 \n","  with torch.no_grad():\n","      for batch in (test_loader):\n","          input_ids=batch['input_ids']\n","          attention_mask=batch['attention_mask']\n","          labels=batch['labels']\n","          outputs=model(input_ids, attention_mask=attention_mask, labels=labels)\n","          predictions=F.softmax(outputs.logits, dim=1)\n","          \"\"\"\n","          pred = []\n","          for x in predictions : \n","            if (x[0].item() >= 0.6 ) : \n","              pred.append(0)\n","            elif (x[2].item() >= 0.6) : \n","              pred.append(2)\n","            else : \n","              pred.append(1)\n","          pred = torch.Tensor(pred)\n","          \"\"\"\n","          pred= torch.argmax(predictions, dim=1)\n","          total += labels.size(0)\n","          test_accuracy += (pred == labels).sum().item()\n","      test_accuracy = (100 * test_accuracy / total)\n","  print(\"test_accuracy:\",test_accuracy )"]},{"cell_type":"code","source":["from transformers import  AutoTokenizer, AutoModelForSequenceClassification\n","PATH = '/content/drive/MyDrive/Colab_Notebooks/classification_camembert/res49/Epoch_5'\n","model = AutoModelForSequenceClassification.from_pretrained(PATH)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"RRArb9MTo1Fr","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["223eb1b7dda74749b5bdc60af8ec96ee","4c6a587366e74f1d90c26d61236ac444","29db9ba85a8b43f4b36454ecc734346a","85568fcf2861407fb42ff5e5701abcf8","d72cb85a97d74aabbd308395c82da5a2","284e5e4d9a3d4595ab39fd0008e907fa","d7ee581af2724b39acb895af9dd0aa29","e65b755454c342c2acb61fd815d06e16","080c9813f8f443dbba50328e273e3384","4a11a20c92124f10868f59886ff6f284","dea12f44f1484369b94d88a4522aa6a6","9cf395df7f704203b3f86169b15d739b","78e98d8399b04b3da1323bf134fbeebd","96f51e8b8924435999a349e524646263","6b4fe3290d734c0aba849fbd1975d9a8","ade3bfd5938745ac964b68549c3e4925","f183c4711a984f01b290c2ac710c3e2d","68d8a3062e714bcdbda0dd2e3746530e","73b4093432fe4b0c867ce5738b9b92e9","d3db5a3c366545e5b03f9666266df9e9","62dff3a3c4474802b56ecc8a05043f43","1807b4b503b34cc4b2c8b43a87109475","b4f2cfe921904402b37218e194927f46","c18373f88ea44976a00f26a223e5dc40","9b01c14604ea48c393154e22a5993b42","4961ab62da3f456c8e56372840622104","7a370b9bc5c0497b96778f69265dd25f","5af59a76363743118b9c04a94e93743b","cc08c7d7e7e845ba9f4dde8d63536d03","1ee1c407544f452cb754dd3c72116ad5","67b1643e2d4c4e58a8fff7f92ff64d19","2636c88b3b9e4a9da068dc6193f13f54","8acd11b625e542b5ad4977705550e7a9"]},"executionInfo":{"status":"ok","timestamp":1653901884271,"user_tz":-120,"elapsed":11635,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"}},"outputId":"d2be8d97-fc07-4f8a-d5da-3607c89e153d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"223eb1b7dda74749b5bdc60af8ec96ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf395df7f704203b3f86169b15d739b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f2cfe921904402b37218e194927f46"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269030,"status":"ok","timestamp":1653858462308,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"F2I2dE6cFR-t","outputId":"c1b368ef-855c-43ff-e774-4df9c5225d73"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/config.json\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1\",\n","  \"architectures\": [\n","    \"CamembertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading weights file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/pytorch_model.bin\n","All model checkpoint weights were used when initializing CamembertForSequenceClassification.\n","\n","All the weights of CamembertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use CamembertForSequenceClassification for predictions without further training.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"camembert-base\",\n","  \"architectures\": [\n","    \"CamembertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading file https://huggingface.co/camembert-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/dbcb433aefd8b1a136d029fe2205a5c58a6336f8d3ba20e6c010f4d962174f5f.160b145acd37d2b3fd7c3694afcf4c805c2da5fd4ed4c9e4a23985e3c52ee452\n","loading file https://huggingface.co/camembert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/84c442cc6020fc04ce266072af54b040f770850f629dd86c5951dbc23ac4c0dd.8fd2f10f70e05e6bf043e8a6947f6cdf9bb5dc937df6f9210a5c0ba8ee48e959\n","loading file https://huggingface.co/camembert-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/camembert-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/camembert-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"camembert-base\",\n","  \"architectures\": [\n","    \"CamembertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading configuration file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/config.json\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1\",\n","  \"architectures\": [\n","    \"CamembertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading weights file /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1/pytorch_model.bin\n","All model checkpoint weights were used when initializing CamembertForSequenceClassification.\n","\n","All the weights of CamembertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use CamembertForSequenceClassification for predictions without further training.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"camembert-base\",\n","  \"architectures\": [\n","    \"CamembertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n","loading file https://huggingface.co/camembert-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/dbcb433aefd8b1a136d029fe2205a5c58a6336f8d3ba20e6c010f4d962174f5f.160b145acd37d2b3fd7c3694afcf4c805c2da5fd4ed4c9e4a23985e3c52ee452\n","loading file https://huggingface.co/camembert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/84c442cc6020fc04ce266072af54b040f770850f629dd86c5951dbc23ac4c0dd.8fd2f10f70e05e6bf043e8a6947f6cdf9bb5dc937df6f9210a5c0ba8ee48e959\n","loading file https://huggingface.co/camembert-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/camembert-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/camembert-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n","Model config CamembertConfig {\n","  \"_name_or_path\": \"camembert-base\",\n","  \"architectures\": [\n","    \"CamembertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 5,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 6,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"camembert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32005\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["train_accuracy: 99.625\n","test_accuracy: 99.75\n"]}],"source":["test_overfit_underfit (X_train,X_test,y_train,y_test,PATH)"]},{"cell_type":"markdown","metadata":{"id":"gELGaTRrsa6M"},"source":["**Test 3+4**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMZIdLXgskNG"},"outputs":[],"source":["def test_contraintes (comments,labels,PATH) : \n","  # Passer comments,labels neutre puis comments,labels de Positive + neutre \n","  test_encoding=tokenizer(comments,truncation=True,padding=True,max_length = 20)\n","  test_dataset=Dataset(test_encoding,labels) \n","  test_loader=DataLoader(test_dataset, batch_size=64, shuffle=True)  \n","  total = 0 \n","  test_accuracy = 0 \n","  with torch.no_grad():\n","      for batch in (test_loader):\n","          input_ids=batch['input_ids']\n","          attention_mask=batch['attention_mask']\n","          labels=batch['labels']\n","          outputs=model(input_ids, attention_mask=attention_mask, labels=labels)\n","          predictions=F.softmax(outputs.logits, dim=1)\n","          \"\"\"\n","          pred = []\n","          for x in predictions : \n","            if (x[0].item() >= 0.6 ) : \n","              pred.append(0)\n","            elif (x[2].item() >= 0.6) : \n","              pred.append(2)\n","            else : \n","              pred.append(1)\n","          pred = torch.Tensor(pred)\n","          \"\"\"\n","          pred= torch.argmax(predictions, dim=1)\n","          total += labels.size(0)\n","          test_accuracy += (pred == labels).sum().item()\n","      test_accuracy = (100 * test_accuracy / total)\n","\n","  print(\"accuracy:\",test_accuracy )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":20898,"status":"error","timestamp":1653858490348,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"Ms7M36Y5GRX3","outputId":"21641149-4433-4ff2-e6de-a0522e43900c"},"outputs":[{"output_type":"stream","name":"stdout","text":["label: 0\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-95b12a4f67ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label:\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_contraintes\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-8a9f5e078488>\u001b[0m in \u001b[0;36mtest_contraintes\u001b[0;34m(comments, labels, PATH)\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m           \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \"\"\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         )\n\u001b[1;32m   1216\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# ATTENTION positive/negative/neutre mta3 train + test  mch \n","for i in range(6):\n","  #df= pd.read_csv(\"/content/drive/MyDrive/Full_Data.csv\")\n","  df1 = df[df['label'] ==i]\n","  df1.dropna(subset=['text'],inplace=True)\n","  comments= (df1.text.tolist())\n","  labels=df1.label.tolist()\n","  print(\"label:\" ,i )\n","  print(test_contraintes (comments,labels,PATH))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hba6aV2GGcnx"},"outputs":[],"source":["# import numpy as np\n","# test_contraintes (comments,labels,PATH)"]},{"cell_type":"markdown","metadata":{"id":"XgwGRNnjqX1b"},"source":["**Test 5 :**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmxfawT_ydjl"},"outputs":[],"source":["import csv\n","def test5(comment) : \n","  # Passer comments,labels neutre puis comments,labels de Positive + neutre \n","  test_encodings=tokenizer([comment], truncation=True, padding=True)\n","  prediction_inputs = torch.tensor(test_encodings['input_ids'])\n","  prediction_masks = torch.tensor(test_encodings['attention_mask'])\n","  with torch.no_grad():\n","    outputs=model(input_ids=prediction_inputs, attention_mask=prediction_masks)\n","    predictions=F.softmax(outputs.logits, dim=1)\n","    print(predictions)\n","    pred = []\n","    for x in predictions : \n","        index=(x== max(x).item()).nonzero(as_tuple=True)[0].item()\n","        #print((x== max(x).item()).nonzero(as_tuple=True)[0].item())\n","        if (index==4 or index==0 or index==1 or index==5) : \n","            return index\n","        elif((index==3 or index==2) and max(x).item()>0.5):\n","          return index\n","        else:\n","          return 6\n","    #pred = torch.Tensor(pred)\"\"\"\n","    #pred1= torch.argmax(predictions, dim=1)\n","    #return pred1\n","#PATH = '/content/drive/MyDrive/Colab_Notebooks/classification_camembert/epochs/Epoch_4'\n","#classes=pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/8classe_updated.csv\")\n","#with open(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/resultat47.csv\", 'w', encoding='UTF8') as f:\n","                    #writer = csv.writer(f)\n","                    #writer.writerow(['text', 'label', 'predicted'])\n","#for i in range(classe.shape[0]):\n","  #print([classe.label[i], test5(classe.text[i]).item()])\n"]},{"cell_type":"markdown","metadata":{"id":"y4tTz4pk6UIA"},"source":["**Test 6**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4821,"status":"ok","timestamp":1653858502233,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"YP6_rCBCn0af","outputId":"3c916701-9463-43a0-f1db-4191d9c3a495"},"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 3.1578947368421053\n"]}],"source":["# text_sentence = \"la france est de dégueulasse \"\n","def test6(comments,labels,PATH) : \n","  # Passer comments,labels neutre puis comments,labels de Positive + neutre \n","  test_encoding=tokenizer(comments,truncation=True,padding=True,max_length = 15)\n","  test_dataset=Dataset(test_encoding,labels) \n","  test_loader=DataLoader(test_dataset, batch_size=64, shuffle=True)  \n","  total = 0 \n","  test_accuracy = 0 \n","  with torch.no_grad():\n","      for batch in (test_loader):\n","          input_ids=batch['input_ids']\n","          attention_mask=batch['attention_mask']\n","          labels=batch['labels']\n","          outputs=model(input_ids, attention_mask=attention_mask, labels=labels)\n","          predictions=F.softmax(outputs.logits, dim=1)\n","          pred= torch.argmax(predictions, dim=1)\n","          total += labels.size(0)\n","          test_accuracy += (pred == labels).sum().item()\n","      test_accuracy = (100 * test_accuracy / total)\n","\n","  print(\"accuracy:\",test_accuracy )\n","\n","classes=pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/8classe_updated.csv\")\n","t= (classes.text.tolist())\n","l=classes.label.tolist()\n","test6(t,l,PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5440,"status":"ok","timestamp":1653858169186,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"qkzX-3n208Rp","outputId":"44a3a338-5774-4d4e-f5e4-88d430498048"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                 text  label\n","0   Organisation mondiale de la Santé (OMS) est l'...      4\n","1   L’importance de la lutte contre les maladies t...      4\n","2   utte contre les maladies transmissibles est au...      4\n","3   il est courant d’affirmer que la charge de mal...      4\n","4   si les campagnes des autorités sanitaires ne s...      4\n","..                                                ...    ...\n","90  Le problème c'est que certaine personnes mal i...      3\n","91  Être laïc, c'est respecter chaque religion et ...      3\n","92  Traiter la laïcité de négative quand on interd...      3\n","93   La loi de 1905 ne comporte pas une fois le te...      3\n","94  Non mais Fillon et ses propos sur les musulman...      3\n","\n","[95 rows x 2 columns]\n","label: 4\n","accuracy: 0.0\n","None\n","label: 0\n","accuracy: 0.0\n","None\n","label: 2\n","accuracy: 0.0\n","None\n","label: 5\n","accuracy: 0.0\n","None\n","label: 1\n","accuracy: 0.0\n","None\n","label: 3\n","accuracy: 0.0\n","None\n"]}],"source":["print(classes)\n","#t=liste.text.tolist()\n","#l=liste.label.tolist()\n","for i in [4,0,2,5,1,3]:\n","  #df= pd.read_csv(\"/content/drive/MyDrive/Full_Data.csv\")\n","  #classes=pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/8classe_updated.csv\")\n","  df1= classes[classes['label'] ==i]\n","  #df1.dropna(subset=['text'],inplace=True)\n","  t= (df1.text.tolist())\n","  l=df1.label.tolist()\n","  print(\"label:\" ,i )\n","  print(test6(t,l,PATH))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37399,"status":"ok","timestamp":1653839476028,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"iymghbrJTn-R","outputId":"99a81fbc-6420-40cf-92f3-e50e7d940014"},"outputs":[{"output_type":"stream","name":"stdout","text":["label: 0\n","accuracy: 61.0\n","None\n","label: 1\n","accuracy: 62.0\n","None\n","label: 2\n","accuracy: 80.0\n","None\n","label: 3\n","accuracy: 62.0\n","None\n","label: 4\n","accuracy: 50.0\n","None\n","label: 5\n","accuracy: 56.0\n","None\n"]}],"source":["classe=pd.read_csv(\"/content/drive/MyDrive/finetune_updated.csv\")\n","for i in range(6):\n","  df1= classe[classe['label'] ==i]\n","  t= (df1.text.tolist())\n","  l=df1.label.tolist()\n","  print(\"label:\" ,i )\n","  print(test6(t,l,PATH))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNZf_7GC6AAp"},"outputs":[],"source":["classes.to_csv(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/8classe_updated.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2-Qg1Tt50Cu"},"outputs":[],"source":["classe.to_csv(\"/content/drive/MyDrive/finetune_updated.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":29877,"status":"error","timestamp":1653902286097,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"eg5oUujQyxQU","outputId":"d7f3f7cc-6c99-411a-ca6e-da5f89453d0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0257, 0.0263, 0.7543, 0.1021, 0.0172, 0.0252, 0.0492]])\n","['zemmour lui a porté la poisse pauvre nicolas il a perdu tout en  mois et en plus il est mis en examen', 2]\n","tensor([[0.0392, 0.0305, 0.6627, 0.1233, 0.0245, 0.0630, 0.0567]])\n","['mr macron à trouvé sont bras droit', 2]\n","tensor([[0.0306, 0.0290, 0.3278, 0.5278, 0.0244, 0.0265, 0.0339]])\n","['il a choisi de mettre lepen comme première opposante lors des législatives à la place de la gauche avec son bras droit communiste elle pose un vrai problème de positionnement sur les terres de nupes', 3]\n","tensor([[0.0281, 0.0275, 0.6598, 0.2031, 0.0193, 0.0243, 0.0379]])\n","[\"en fait c'est macron lui-même le vrai premier ministre de la france\", 2]\n","tensor([[0.0302, 0.0291, 0.5065, 0.3454, 0.0223, 0.0236, 0.0428]])\n","[\"pauvre macron il est à la recherche d'un  ministre grandeur et décadence on ne créer pas la grandeur d'une nation avec de piètres politiciens\", 2]\n","tensor([[0.6577, 0.0404, 0.0422, 0.0460, 0.0862, 0.0348, 0.0927]])\n","['pour favoriser une cadre de la fi', 0]\n","tensor([[0.5387, 0.0400, 0.1114, 0.0721, 0.0863, 0.0333, 0.1182]])\n","['macron va creer un ministere de soutien aux deserteurs', 0]\n","tensor([[0.1375, 0.1060, 0.1137, 0.2028, 0.0560, 0.0428, 0.3411]])\n","['que des macron compatibles', 6]\n","tensor([[0.1510, 0.0392, 0.5704, 0.1011, 0.0255, 0.0300, 0.0828]])\n","['ils ont passé un accord avec macron avant le  tour ils ont vendu la cause des ouvriers et veulent entraîner le peuple dans un chaos qu ils ont préparé', 2]\n","tensor([[0.0378, 0.0337, 0.3661, 0.4212, 0.0256, 0.0287, 0.0869]])\n","[\"dans un moment de lucidité et ayant peur de perdre son âme le ps rejette l'alliance avec l'extrême gauche de mélenchon ouf\", 6]\n","tensor([[0.0771, 0.0715, 0.1223, 0.0640, 0.0391, 0.0566, 0.5693]])\n","['vu le score de mélenchon au  tour nous pouvons mesurer la hauteur des lfi', 6]\n","tensor([[0.0690, 0.0706, 0.1689, 0.0712, 0.0351, 0.0560, 0.5292]])\n","['il avais pas eu besoin de monter sa liste contre marine au  er tour il a ce qui merite', 6]\n","tensor([[0.0338, 0.0262, 0.6951, 0.1522, 0.0212, 0.0201, 0.0515]])\n","['et puis quoi encore je rêve à la retraite mélenchon', 2]\n","tensor([[0.3468, 0.0326, 0.2262, 0.1197, 0.1707, 0.0317, 0.0723]])\n","[\"macron devient à nouveau président pour ma part j'ai bien peur que la protection sociale toute entière soit en danger de disparition\", 0]\n","tensor([[0.1056, 0.0852, 0.0786, 0.0787, 0.0671, 0.0599, 0.5249]])\n","['plus jamais ps', 6]\n","tensor([[0.0287, 0.0272, 0.1973, 0.6576, 0.0244, 0.0240, 0.0409]])\n","[\"marine vous avez raison des propos mensonger c'est tous le jours moi c'est marine\", 3]\n","tensor([[0.0432, 0.0364, 0.5636, 0.2535, 0.0254, 0.0286, 0.0493]])\n","['oui si vous voulez mais il y en aurait tout autant au minimum de ne pas choisir macron', 2]\n","tensor([[0.0310, 0.0293, 0.2264, 0.6091, 0.0277, 0.0331, 0.0434]])\n","[\"ce qui nous importe c'est un oui à  des voix pour marine le pen\", 3]\n","tensor([[0.1087, 0.1286, 0.0792, 0.0775, 0.0658, 0.0657, 0.4745]])\n","['allez marine', 6]\n","tensor([[0.0623, 0.0462, 0.2937, 0.3319, 0.0395, 0.0609, 0.1656]])\n","[' marine le pen présidente', 6]\n","tensor([[0.0280, 0.0241, 0.1051, 0.7586, 0.0309, 0.0239, 0.0294]])\n","[\"l'apparente modestie de mélenchon où ça la république c'est moi c'est modeste ça\", 3]\n","tensor([[0.0280, 0.0256, 0.6130, 0.2557, 0.0197, 0.0236, 0.0345]])\n","[\"tout les petits journaliste de la france sont avec macron mais il n'ont pas honte ses français\", 2]\n","tensor([[0.0272, 0.0287, 0.5879, 0.2555, 0.0194, 0.0261, 0.0553]])\n","[\"je n'ai pas changer d'avis devant un président menteur odieux et sa posture avachie sur la table en dit long donneur de leçons ce sera marine\", 2]\n","tensor([[0.0434, 0.0359, 0.6210, 0.1775, 0.0226, 0.0266, 0.0730]])\n","['jamais vu un président sortant aussi méprisant que macron', 2]\n","tensor([[0.0617, 0.3544, 0.0809, 0.0766, 0.0474, 0.0716, 0.3073]])\n","['super marine', 1]\n","tensor([[0.0347, 0.0288, 0.3525, 0.2947, 0.2087, 0.0414, 0.0392]])\n","['lui il est comme macron aussi pouri faut pas oublier qu il a dit j irait chercher les noms vacciné chez eux pour les piquer et vient', 6]\n","tensor([[0.0283, 0.0229, 0.1209, 0.7342, 0.0330, 0.0283, 0.0324]])\n","['les médias sont devenus pro melenchon', 3]\n","tensor([[0.0362, 0.0313, 0.2986, 0.5309, 0.0266, 0.0309, 0.0454]])\n","['non nous ne sommes pas les électeurs de jean luc mélenchon nous sommes des citoyens nous votons en âmes et conscience merci', 3]\n","tensor([[0.0623, 0.0462, 0.2937, 0.3319, 0.0395, 0.0609, 0.1656]])\n","['marine le pen présidente', 6]\n","tensor([[0.0286, 0.0249, 0.1842, 0.6555, 0.0301, 0.0333, 0.0433]])\n","['vive marine le pen', 3]\n","tensor([[0.0483, 0.0348, 0.3806, 0.4117, 0.0270, 0.0338, 0.0638]])\n","['macron aurait dû faire ce discours le  avril', 6]\n","tensor([[0.0465, 0.0365, 0.5640, 0.1972, 0.0195, 0.0265, 0.1098]])\n","['faut il que macron et ses disciples soient effrayés pour user au dernier moment de tels artifices', 2]\n","tensor([[0.0358, 0.0339, 0.6166, 0.1861, 0.0224, 0.0287, 0.0765]])\n","[\"je n'ai jamais voté pour elle mais je trouve ce procédé à vomir macron a des casseroles terribles mais ça passe sous silence car les médias sont aux ordres après ils s'étonnent que le peuple ne veut plus entendre parler d'eux\", 2]\n","tensor([[0.0340, 0.0293, 0.6544, 0.1939, 0.0222, 0.0270, 0.0393]])\n","['comme par hasard on peut parler aussi de macron avec toutes les affaires judiciaires pendant son mandat', 2]\n","tensor([[0.0421, 0.0388, 0.6076, 0.2002, 0.0205, 0.0285, 0.0623]])\n","[\"j'apprécie beaucoup emmanuel macron c'est vraiment le meilleur chef d'état de la galaxie\", 2]\n","tensor([[0.0615, 0.0297, 0.2095, 0.5819, 0.0414, 0.0244, 0.0516]])\n","['emmanuel macron pour  ans', 3]\n","tensor([[0.0482, 0.0354, 0.5854, 0.1420, 0.0250, 0.0262, 0.1379]])\n","[\"ils vont vous dire qu'ils votent rn parce qu'ils connaissent bien les gens que le rn rejette\", 2]\n","tensor([[0.0339, 0.0295, 0.6531, 0.1944, 0.0200, 0.0243, 0.0448]])\n","[\"vu les  ans qu'on a eues avec macron pour les  prochaines années encore avec lui oui\", 2]\n","tensor([[0.0618, 0.3351, 0.1025, 0.1565, 0.0710, 0.0459, 0.2272]])\n","['c’est quoi le nupes', 1]\n","tensor([[0.0829, 0.0572, 0.1990, 0.1436, 0.0372, 0.0508, 0.4292]])\n","['lrem devra payer çà cash dans les urnes', 6]\n","tensor([[0.0346, 0.0267, 0.3693, 0.4499, 0.0310, 0.0316, 0.0568]])\n","[\"personne n'a jamais retrouvé aucune trace d'adhésion au ps elle était conseillère dans des ministères avec à leur tête des socialistes nuance elle n'a jamais milité\", 6]\n","tensor([[0.0311, 0.0270, 0.7597, 0.1044, 0.0179, 0.0189, 0.0409]])\n","['la gauche europeiste macronienne est elle vraiment de gauche pour les slovaques polonais roumains et ukrainiens peut-être…les socialistes ont voté macron au  tour de  et …', 2]\n","tensor([[0.0339, 0.0271, 0.6189, 0.2177, 0.0279, 0.0323, 0.0422]])\n","[\"elle n'est qu'une marionnette contrôlée par les plus proches collaborateurs de macron et n'est que pour faire valoir en tant que femme\", 2]\n","tensor([[0.0352, 0.0239, 0.2813, 0.5479, 0.0374, 0.0306, 0.0438]])\n","['il faut absolument battre et faire battre les député lrem', 3]\n","tensor([[0.0435, 0.0242, 0.4196, 0.4176, 0.0283, 0.0217, 0.0451]])\n","[\"c'est un choix satisfait à la gauche de mélenchon et ceux de la gauche caviar d'aubergine\", 6]\n","tensor([[0.0272, 0.0222, 0.1374, 0.7262, 0.0295, 0.0268, 0.0307]])\n","['la république exemplaire façon macron', 3]\n","tensor([[0.0287, 0.0249, 0.1469, 0.7116, 0.0280, 0.0242, 0.0357]])\n","[\"une image de gauchemais c'est l'image même de lrem ultra-liberal enlevez le mot gauche svp\", 3]\n","tensor([[0.0381, 0.0286, 0.5288, 0.2538, 0.0437, 0.0239, 0.0831]])\n","['ça c’est un coup de macron avec sa poudre de perlimpinpin ￼', 2]\n","tensor([[0.0367, 0.0298, 0.6727, 0.1748, 0.0192, 0.0221, 0.0447]])\n","['merci macron grâce vous le boulot ce trouve sur le trottoir d’en face', 2]\n","tensor([[0.0441, 0.0338, 0.6195, 0.1912, 0.0211, 0.0294, 0.0609]])\n","['merci macron ce génie', 2]\n","tensor([[0.5286, 0.0443, 0.2006, 0.0671, 0.0480, 0.0299, 0.0815]])\n","[\"oui mais avant l'election de macron il n'y avait presque plus de chomage encore ils nous font croire des balivernes et les gens godent\", 0]\n","tensor([[0.1726, 0.0340, 0.5991, 0.0708, 0.0347, 0.0267, 0.0621]])\n","['bien sûr tout les intérimaires inscrit qui ont l âge de la retraite ils les pousse vers la sortie beaucoup de monde ne le sait pas ça vive macron', 2]\n","tensor([[0.0816, 0.0644, 0.1196, 0.0738, 0.0431, 0.0529, 0.5646]])\n","['il aurait dû appeler son partie recyclage plutôt que renaissance', 6]\n","tensor([[0.0305, 0.0253, 0.5484, 0.3099, 0.0243, 0.0265, 0.0351]])\n","['la bonne personne à la bonne place bien joué monsieur macron tous mes voeux de réussite pour cette pupille de la nation méritante et compétente un symbole de la méritocratie républicaine', 2]\n","tensor([[0.0305, 0.0253, 0.5484, 0.3099, 0.0243, 0.0265, 0.0351]])\n","['la bonne personne à la bonne place bien joué monsieur macron tous mes voeux de réussite pour cette pupille de la nation méritante et compétente un symbole de la méritocratie républicaine', 2]\n","tensor([[0.0479, 0.0274, 0.6570, 0.1757, 0.0215, 0.0207, 0.0498]])\n","['macron tu as dépassé les bornes', 2]\n","tensor([[0.1184, 0.0625, 0.0942, 0.2051, 0.0597, 0.0454, 0.4146]])\n","['et chez renaissance ça se passe comment', 6]\n","tensor([[0.0323, 0.0246, 0.0800, 0.7618, 0.0354, 0.0256, 0.0403]])\n","[\"pourquoi préciser eelv comme s'est bizarre\", 3]\n","tensor([[0.0419, 0.0297, 0.5457, 0.2905, 0.0204, 0.0239, 0.0477]])\n","[\"avec macron il y a les paroles qui n'engagent jamais les actes\", 2]\n","tensor([[0.0333, 0.0310, 0.6374, 0.1933, 0.0204, 0.0279, 0.0566]])\n","[\"un pantin qui ne devait pas faire d'ombre à macron\", 2]\n","tensor([[0.0321, 0.0265, 0.0663, 0.7675, 0.0382, 0.0303, 0.0391]])\n","['en dehors de larem pas de salut', 3]\n","tensor([[0.0553, 0.0354, 0.2740, 0.4023, 0.0371, 0.0296, 0.1663]])\n","[\"bon courage pour l'après radiation du ps et l'après déroute électorale\", 6]\n","tensor([[0.0280, 0.0246, 0.1273, 0.7370, 0.0296, 0.0248, 0.0286]])\n","['le ps en dordogneon sait ce que cela donnederrière la droite macronistela plupart du temps', 3]\n","tensor([[0.0333, 0.0301, 0.6993, 0.1384, 0.0170, 0.0230, 0.0589]])\n","[\"mélenchon ne vient jamais à marseille de toute façon il s'en fout\", 2]\n","tensor([[0.0507, 0.0353, 0.2898, 0.4271, 0.0360, 0.0332, 0.1277]])\n","[\"c'était l'article anti mélenchon du jour à demain\", 6]\n","tensor([[0.1029, 0.0375, 0.4426, 0.3161, 0.0290, 0.0224, 0.0494]])\n","['merci anne hidalgo perso je ne viens plus à paris pour acheter quoi que ce soit', 6]\n","tensor([[0.0368, 0.0376, 0.7165, 0.1217, 0.0207, 0.0301, 0.0365]])\n","['on ne peux plus se garer ds paris ni circuler normal que les commerces ferment hidalgo responsable', 2]\n","tensor([[0.0515, 0.0282, 0.4947, 0.3439, 0.0236, 0.0220, 0.0361]])\n","[\"hidalgo à tout fait pour qu'il en soit ainsielle n'a jamais était capable de comprendre qu'elle cassait le commerce\", 6]\n","tensor([[0.0379, 0.0679, 0.3147, 0.2706, 0.0226, 0.1838, 0.1024]])\n","['merci anne d’avoir détruit paris', 6]\n","tensor([[0.0843, 0.0471, 0.6425, 0.0796, 0.0248, 0.0250, 0.0969]])\n","['et surtout difficile de se garer dans paris merci hidalgo les gens fuient la capitale pour faire les courses', 2]\n","tensor([[0.1291, 0.0790, 0.4002, 0.2397, 0.0352, 0.0330, 0.0838]])\n","[\"alors quand il n'y aura plus que des vélosvoitures interditesles commerçants fuiront paris qui dira merci à hidalgo\", 6]\n","tensor([[0.0601, 0.0344, 0.6388, 0.1469, 0.0198, 0.0200, 0.0799]])\n","['merci quic est plutôt la crise de la gestion de paris par hidalgo', 2]\n","tensor([[0.0746, 0.0566, 0.1276, 0.2517, 0.0525, 0.0590, 0.3780]])\n","['merci hidalgo', 6]\n","tensor([[0.1522, 0.0355, 0.1555, 0.4889, 0.0624, 0.0259, 0.0796]])\n","['la crise de l’habillement non la crise c’est hidalgo', 6]\n","tensor([[0.0430, 0.0330, 0.3248, 0.3955, 0.0305, 0.0350, 0.1382]])\n","['bravo hidalgo', 6]\n","tensor([[0.0305, 0.0266, 0.7376, 0.1335, 0.0185, 0.0198, 0.0333]])\n","['c est pas la crise c est hidalgo qui fait fuir les gens', 2]\n","tensor([[0.0435, 0.0499, 0.7125, 0.1098, 0.0187, 0.0272, 0.0384]])\n","['ben oui hidalgo prélèvera moins d impôts pour détruire paris', 2]\n","tensor([[0.1774, 0.0460, 0.5730, 0.0937, 0.0298, 0.0243, 0.0559]])\n","['les gens depensent autrementet hidalgo a banni la classe moyenne', 2]\n","tensor([[0.0331, 0.0339, 0.6618, 0.1856, 0.0198, 0.0274, 0.0385]])\n","['ce qui a fait le plus de tort à paris c’est hidalgo sa politique et les bobos parisiens qui la soutiennent', 2]\n","tensor([[0.0392, 0.0303, 0.5603, 0.2616, 0.0245, 0.0442, 0.0398]])\n","['c’est hidalgo qui a tué les commerces à paris', 2]\n","tensor([[0.0303, 0.0324, 0.4599, 0.3336, 0.0202, 0.0415, 0.0821]])\n","['hidalgo fait fuir', 6]\n","tensor([[0.1114, 0.0739, 0.3632, 0.0921, 0.0314, 0.0351, 0.2928]])\n","[\"avant j'allais au ciné au théâtre au resto et faire du shopping dans paris mais ça c'était avant avant hidalgo et ses travaux à tout va\", 6]\n","tensor([[0.0267, 0.0310, 0.4460, 0.4059, 0.0248, 0.0358, 0.0298]])\n","['pourquoi on ne fout pas dehors hidalgo elle massacre la capitale', 6]\n","tensor([[0.0307, 0.0253, 0.1445, 0.7015, 0.0296, 0.0235, 0.0447]])\n","['vive hidalgo', 3]\n","tensor([[0.0443, 0.0292, 0.6598, 0.1418, 0.0194, 0.0250, 0.0805]])\n","['macron ne les a pas sauvés', 2]\n","tensor([[0.0466, 0.0404, 0.6571, 0.1246, 0.0217, 0.0229, 0.0865]])\n","[\"résultats d'une politique égocentrique du seul ps qui n'a rien fait avancer depuis  ans dans cette région\", 2]\n","tensor([[0.0467, 0.0322, 0.7542, 0.0869, 0.0188, 0.0198, 0.0413]])\n","[\"macron le trader a tout acheté dans ce pays a tout chamboulé a tout éclaté a tout cassé pour faire quoi de ce pays aujourd'hui devenu autocratique\", 2]\n","tensor([[0.0343, 0.0312, 0.4843, 0.3109, 0.0219, 0.0240, 0.0934]])\n","[\"on est pas dupe des nupes ceux qui croient qu'en amalgamant les losers ils vont devenir bons\", 6]\n","tensor([[0.0449, 0.0305, 0.7623, 0.0813, 0.0180, 0.0204, 0.0426]])\n","[\"voilà macron trader il a tout cassé à son seul profita qui va-t-il vendre le pays maintenant qu'il est seul au pouvoir\", 2]\n","tensor([[0.0333, 0.0257, 0.7172, 0.1494, 0.0177, 0.0192, 0.0374]])\n","[\"pourtant macron aimerait tant nous imposer l'esclavage c'est son rêve\", 2]\n","tensor([[0.0481, 0.0280, 0.3961, 0.3446, 0.0360, 0.0279, 0.1193]])\n","['pauvre ps', 6]\n","tensor([[0.0720, 0.0763, 0.1482, 0.0831, 0.0340, 0.0498, 0.5365]])\n","[\"les éléphants n'existent pas qu'au ps\", 6]\n","tensor([[0.0288, 0.0263, 0.3946, 0.4502, 0.0230, 0.0270, 0.0500]])\n","[\"j'aime beaucoup cette femme mais elle a passé son temps à humilier mélenchon\", 6]\n","tensor([[0.0306, 0.0270, 0.2938, 0.5612, 0.0238, 0.0231, 0.0406]])\n","[\"homme fort de gauche non il finira avec lo et npa seulje l'espère il reste encore des démocrates à gauche malgré le mépris dont ils ont fait l'objet depuis \", 3]\n","tensor([[0.0273, 0.0228, 0.5981, 0.2591, 0.0204, 0.0251, 0.0472]])\n","['il faut arrêter de nous abreuver des informations concernant larem ou bien reconquête ou bien ce que vouvoyez mais stop ça suffit', 2]\n","tensor([[0.1087, 0.0369, 0.6784, 0.0708, 0.0218, 0.0205, 0.0630]])\n","[\"le trader macron qui après avoir cassé la france veut casser l'europe\", 2]\n","tensor([[0.1092, 0.0595, 0.0953, 0.1316, 0.0413, 0.0908, 0.4723]])\n","['la renaissance des notables', 6]\n","tensor([[0.1463, 0.0931, 0.2215, 0.0996, 0.0448, 0.0462, 0.3485]])\n","['elle ne cible pas macron', 6]\n","tensor([[0.0278, 0.0216, 0.1637, 0.6953, 0.0311, 0.0289, 0.0316]])\n","[\"bravooo vous avez raisonc'est vous la député légitimepas une fi\", 3]\n","tensor([[0.0304, 0.0258, 0.2946, 0.5686, 0.0237, 0.0219, 0.0350]])\n","['nupes pour les dupes les signataires de cet accord contre nature sont des renégatsbravo aux démocrates qui par respect pour leurs électeurs ne se laissent pas imposer une soumission contre nature avec les extrêmes fi', 3]\n","tensor([[0.1033, 0.0769, 0.2611, 0.4483, 0.0308, 0.0259, 0.0537]])\n","['le nupes des dupesnombreux sont les candidats démocrates qui refusent cette soumission aux extrêmes dans une alliance contre-natureles signataires de cet accord sont des renégats', 6]\n","tensor([[0.0600, 0.0609, 0.1713, 0.2309, 0.0347, 0.0473, 0.3949]])\n","['le nupes des dupes très belle tribune tout est dit', 6]\n","tensor([[0.0656, 0.0348, 0.5341, 0.2556, 0.0219, 0.0234, 0.0644]])\n","['oui mais depuis ils sont passés sous le joug macron', 2]\n","tensor([[0.1618, 0.0840, 0.1237, 0.1241, 0.0597, 0.0594, 0.3873]])\n","['que pèse  millions de voix comparés aux  millions de marine', 6]\n","tensor([[0.0782, 0.0642, 0.1393, 0.0970, 0.0402, 0.0513, 0.5297]])\n","[\"qu'on attendra la prochaine élection avant d'enterrer définitivement le ps\", 6]\n","tensor([[0.0430, 0.0376, 0.4249, 0.3322, 0.0220, 0.0231, 0.1172]])\n","['c’est comme l’apparence lrem renaissance modem et autres satellites…vouloir renverser la table pour faire des âneries pareilles pffffff', 6]\n","tensor([[0.1033, 0.0845, 0.0590, 0.0643, 0.0598, 0.0784, 0.5507]])\n","[\"autant qu'à renaissance\", 6]\n","tensor([[0.0447, 0.0321, 0.1099, 0.6219, 0.0493, 0.0400, 0.1022]])\n","['non pas le ps le bureau national du ps', 3]\n","tensor([[0.0506, 0.0303, 0.1685, 0.3580, 0.2331, 0.0394, 0.1201]])\n","['renaissance à la maison de retraite ça swing', 6]\n","tensor([[0.0928, 0.1015, 0.1050, 0.2456, 0.0566, 0.0474, 0.3511]])\n","['renaissance immobile', 6]\n","tensor([[0.1374, 0.0434, 0.2195, 0.1975, 0.2473, 0.0836, 0.0714]])\n","['ben difficile de trouver un ou une premier-e ministre pour mettre en marche sa retraite à  ans voire à  ans comme le préconise edouard philippe allié à macron', 4]\n","tensor([[0.0953, 0.0850, 0.0693, 0.0499, 0.0560, 0.0789, 0.5656]])\n","['les plus anciens du ps', 6]\n","tensor([[0.0274, 0.0270, 0.5223, 0.3447, 0.0215, 0.0228, 0.0343]])\n","[\"il n'y a aucune ambiguïté de mélenchon sur la question identitaire seul les raciste et xénophobe sont inquiets de ça\", 2]\n","tensor([[0.2452, 0.0459, 0.4865, 0.0881, 0.0340, 0.0284, 0.0720]])\n","['donc ils sont inquiets tant que ce n’est pas macron et sa clique en faitfaut dire que l’austérité leur réussit alors ce serait dommage de changer de méthode pour le bien du peuple', 6]\n","tensor([[0.0292, 0.0271, 0.5098, 0.3472, 0.0215, 0.0274, 0.0377]])\n","['ah ben oui car avec melenchon il y aura un nouveau fureurje le vois déjà faire des discours enflammés à nurembergpardon je voulais dire à bruxelles éructer contre les ennemis du peuple qu il faudra mettre dans des camps de longues vacancespour les réeduquer', 2]\n","tensor([[0.0625, 0.0294, 0.2197, 0.5334, 0.0394, 0.0327, 0.0828]])\n","['il y a longtemps que ce parti est divisé entre ceux qui vont vers lrem et ceux qui restent socialistes', 3]\n","tensor([[0.0335, 0.0263, 0.6480, 0.2109, 0.0185, 0.0199, 0.0429]])\n","[\"les socialos de droite vont rejoindre macron tant mieux pas besoin d'eux\", 2]\n","tensor([[0.0353, 0.0268, 0.1777, 0.6497, 0.0342, 0.0297, 0.0466]])\n","['il faut reconstruire un autre ps avec tous ceux qui non pas renonce à notre idéal pour sauver quelques mandats de députés', 3]\n","tensor([[0.1024, 0.0729, 0.0923, 0.0659, 0.0570, 0.0584, 0.5510]])\n","['rip ps français', 6]\n","tensor([[0.0285, 0.0252, 0.1079, 0.7529, 0.0311, 0.0222, 0.0322]])\n","[\"la culture des courantsceux qui se sont soumis à lrem et aboientceux qui ne parlent que du passé et de lamententceux qui s'accrochent à \", 3]\n","tensor([[0.0304, 0.0323, 0.6377, 0.2079, 0.0209, 0.0269, 0.0440]])\n","['il est plus sérieux que melenchon qui est un polichinelle', 2]\n","tensor([[0.0572, 0.0308, 0.6427, 0.1706, 0.0225, 0.0235, 0.0528]])\n","['au revoir et sans retour ces pseudo socialistes ont fait la même politique que macron et ont alimenté le mécontentement populaire', 2]\n","tensor([[0.0413, 0.0285, 0.3433, 0.4847, 0.0280, 0.0272, 0.0469]])\n","[\"en le quittant il rend service au ps il y en a bien d'autres qui devraient faire de même pour que ce ps puisse enfin faire une vraie politique de gauche\", 6]\n","tensor([[0.0920, 0.0772, 0.0883, 0.2560, 0.0535, 0.0544, 0.3787]])\n","['le ps n’existe plus et cela ne date pas d’hier …', 6]\n","tensor([[0.0701, 0.0592, 0.1302, 0.0774, 0.1483, 0.0735, 0.4414]])\n","['il va aller chez lrem', 6]\n","tensor([[0.0328, 0.0252, 0.1563, 0.6721, 0.0314, 0.0257, 0.0565]])\n","['pour ce que le ps représente et ce que représente caseneuve', 3]\n","tensor([[0.0323, 0.0264, 0.4485, 0.3570, 0.0298, 0.0492, 0.0569]])\n","[\"celui que tant de mes amis restés au ps espéraient voir se porter candidat à cette présidentielle  le parti décision estimable de cet figure du ps respectéed'autres ne manqueront pas de faire comme lui\", 6]\n","tensor([[0.0938, 0.0734, 0.0726, 0.0493, 0.0460, 0.0693, 0.5956]])\n","[\"le ps s'en remettra\", 6]\n","tensor([[0.0696, 0.0728, 0.1158, 0.0878, 0.0396, 0.0618, 0.5525]])\n","['il a participé au torpillage du ps ce nul', 6]\n","tensor([[0.7025, 0.0379, 0.0539, 0.0470, 0.0565, 0.0324, 0.0698]])\n","[\"dans les transactions il y a toujours des concessions de part et d'autre c'est normal et pour notre interet a tous notamment pour contrer la retraite a  ans de macron\", 0]\n","tensor([[0.0381, 0.0272, 0.3670, 0.4708, 0.0245, 0.0253, 0.0470]])\n","[\"avec l'unité et la raison de toutes les forces progressistes de la gauche nous vaincrons et barrons la route à macron ️️️\", 6]\n","tensor([[0.0329, 0.0244, 0.1609, 0.6895, 0.0296, 0.0211, 0.0416]])\n","[\"je comprends pas pourquoi le ps donc la droite irait s'accorder avec fi et les communistes\", 3]\n","tensor([[0.0926, 0.1561, 0.0606, 0.1603, 0.1576, 0.0805, 0.2924]])\n","['nupes', 6]\n","tensor([[0.0342, 0.0293, 0.4214, 0.4139, 0.0220, 0.0231, 0.0562]])\n","[\"les éléphants qui menacent de quitter le ps ça prouve bien qu'ils ne sont pas de gauche\", 6]\n","tensor([[0.0668, 0.0598, 0.1035, 0.2550, 0.0561, 0.0581, 0.4007]])\n","['qui annonce çà au ps', 6]\n","tensor([[0.0446, 0.0312, 0.2322, 0.5599, 0.0288, 0.0256, 0.0778]])\n","['le ps était toujours leader des  de gauche avec ces discutions dur sera d’accepter d’être au second voir  rangs', 3]\n","tensor([[0.0441, 0.0288, 0.5599, 0.2693, 0.0274, 0.0282, 0.0423]])\n","['des jaloux proches de macron qui vont se faire balayer aux législatives', 2]\n","tensor([[0.6246, 0.0399, 0.0451, 0.0418, 0.0599, 0.0469, 0.1418]])\n","['prochaine étape transformer le siège du pcf place du colonel fabien en musée', 0]\n","tensor([[0.1301, 0.0641, 0.0662, 0.1032, 0.0616, 0.0554, 0.5194]])\n","['le premier e de eelv vient de tomber et la suite', 6]\n","tensor([[0.0702, 0.0345, 0.0438, 0.2794, 0.1692, 0.3475, 0.0554]])\n","['m jadot prévoit une intervention médiatique', 5]\n","tensor([[0.0769, 0.0224, 0.1785, 0.3380, 0.2750, 0.0330, 0.0762]])\n","['enfin une bonne nouvelle de la rem', 6]\n","tensor([[0.2008, 0.0526, 0.0945, 0.1860, 0.0587, 0.0413, 0.3661]])\n","[\"un accord avec l'extrème gauche de mélenchon signerait la fin du ps\", 6]\n","tensor([[0.0658, 0.4310, 0.1971, 0.1695, 0.0313, 0.0355, 0.0698]])\n","['la notion d’écologie pour anne hidalgo est soit un mystère soit une absurdité pauvres parisiens', 1]\n","tensor([[0.0795, 0.0475, 0.1625, 0.2871, 0.0507, 0.0511, 0.3216]])\n","['lrem irréprochables', 6]\n","tensor([[0.0301, 0.0260, 0.0925, 0.7648, 0.0313, 0.0233, 0.0320]])\n","[\"mélenchon le franc maçon qui pousse sa gueulante dès qu'on le contredit\", 3]\n","tensor([[0.0485, 0.0288, 0.6635, 0.1236, 0.0237, 0.0222, 0.0898]])\n","['décidément tout se fait en famille la dynastie lepen est bonne pour une rentière accrochée à son fauteuil', 2]\n","tensor([[0.0317, 0.0238, 0.1117, 0.7353, 0.0351, 0.0276, 0.0347]])\n","['neutralisons le pas un seul député lrem', 3]\n","tensor([[0.0303, 0.0243, 0.4511, 0.4125, 0.0233, 0.0210, 0.0375]])\n","[\"l'union et vite il faut neutraliser macron\", 6]\n","tensor([[0.0301, 0.0260, 0.0925, 0.7648, 0.0313, 0.0233, 0.0320]])\n","[\"mélenchon le franc maçon qui pousse sa gueulante dès qu'on le contredit\", 3]\n","tensor([[0.0784, 0.0842, 0.4682, 0.1757, 0.0205, 0.0253, 0.1477]])\n","['le changement c est macron', 6]\n","tensor([[0.0269, 0.0256, 0.1212, 0.7364, 0.0272, 0.0249, 0.0378]])\n","[\"il a plus peur de la fin du ps que de la fin de l'humanité\", 3]\n","tensor([[0.4991, 0.1144, 0.0629, 0.0339, 0.0488, 0.0354, 0.2055]])\n","['ils sont déjà raccordés à lrem', 0]\n","tensor([[0.0400, 0.0239, 0.1576, 0.6684, 0.0271, 0.0258, 0.0573]])\n","['le ps pris au piège des melanchontistes', 3]\n","tensor([[0.0483, 0.0401, 0.5711, 0.1318, 0.0207, 0.0281, 0.1598]])\n","[\"il faut l'analyser et voir pourquoi ils sont aussi remontes contre macron\", 2]\n","tensor([[0.0833, 0.0767, 0.0860, 0.0568, 0.0454, 0.0667, 0.5851]])\n","['bravo au ps', 6]\n","tensor([[0.0480, 0.0252, 0.3345, 0.4778, 0.0277, 0.0234, 0.0634]])\n","[\"le ps mais ils vont négocier quoi il n'existe plus dans les urnes\", 6]\n","tensor([[0.0324, 0.0301, 0.5687, 0.2799, 0.0213, 0.0263, 0.0412]])\n","['il ne faut surtout pas que macron obtienne la majorité absolue relative pour qu il compose avec d autres que ces lrem godillots', 2]\n","tensor([[0.0322, 0.0258, 0.6583, 0.2025, 0.0199, 0.0235, 0.0379]])\n","['vous prenez vos désirs pour des réalitésraisons de plus pour allez voter en masse à nouveau contre macron', 2]\n","tensor([[0.0325, 0.0255, 0.6206, 0.2380, 0.0201, 0.0210, 0.0422]])\n","['nous le peuple en bas de la pyramide on s en fou de la guerre du trôneon espère que macron ne finisse pas son mandat', 2]\n","tensor([[0.0276, 0.0248, 0.1552, 0.7035, 0.0292, 0.0247, 0.0349]])\n","['et oui édouard philippe prochain président dans  ans', 3]\n","tensor([[0.0395, 0.0404, 0.5278, 0.1946, 0.0213, 0.0294, 0.1470]])\n","['vous feriez mieux de vous alliés avec zemmour', 2]\n","tensor([[0.0334, 0.0243, 0.6462, 0.2137, 0.0215, 0.0236, 0.0374]])\n","['encore heureux la dynastie lepen est de plus en plus insupportable et elle a généré trop de division', 2]\n","tensor([[0.0285, 0.0287, 0.5404, 0.3000, 0.0185, 0.0229, 0.0610]])\n","['très bien de toute façon il y a pas besoin de se cachervive marine', 2]\n","tensor([[0.0952, 0.0739, 0.0694, 0.0537, 0.0835, 0.0563, 0.5680]])\n","['il copie encore une fois le programme de mlp', 6]\n","tensor([[0.0628, 0.1477, 0.2107, 0.1229, 0.0358, 0.0544, 0.3657]])\n","['vive marine', 6]\n","tensor([[0.0319, 0.0251, 0.4544, 0.4008, 0.0238, 0.0245, 0.0395]])\n","[\"qu'est ce qu'il faut pas lire et entendre vivement dimanche qu'on en finisse et qu'on retrouve le macron qu'on connait l homme de droite qui casse insulte les enseignants et j en passe\", 6]\n","tensor([[0.0351, 0.0274, 0.7103, 0.1512, 0.0188, 0.0192, 0.0380]])\n","['unie derrière les riches non merci tout sauf macron', 2]\n","tensor([[0.0431, 0.0378, 0.5228, 0.2747, 0.0238, 0.0311, 0.0668]])\n","[\"macron n'a pas voulu de campagne pour etre certain d'etre réélu de ce fait les sujets essentiels n'ont pascete abordes il a gagné a ce jeu là c'est pourquoi je vote blanc\", 2]\n","tensor([[0.0338, 0.0263, 0.4650, 0.3909, 0.0215, 0.0211, 0.0414]])\n","[\"média pro macron c'est sûr il faut préserver les subventions beurkkkkk\", 6]\n","tensor([[0.0346, 0.0473, 0.3085, 0.2612, 0.0345, 0.2506, 0.0634]])\n","['ou l inverse vu que la violence avec macron a déjà fait partie de son mandat', 6]\n","tensor([[0.0528, 0.0265, 0.6767, 0.1470, 0.0207, 0.0225, 0.0538]])\n","['le choix est limpide macron', 2]\n","tensor([[0.0551, 0.0375, 0.5238, 0.2323, 0.0308, 0.0388, 0.0818]])\n","[\"pour cela il aurait fallu que l'union populaire soit au deuxième tour et mélenchon élu président\", 2]\n","tensor([[0.0303, 0.0285, 0.5829, 0.2719, 0.0197, 0.0211, 0.0455]])\n","[\"macron est d'une arrogance absolue tout sauf macron tout\", 2]\n","tensor([[0.0386, 0.0311, 0.6186, 0.2163, 0.0274, 0.0272, 0.0407]])\n","['cette idée de mélenchon premier ministre est ridicule on ne fait pas une majorité parlementaire avec  des voix comme en  les électeurs donneront une majorité au président', 2]\n","tensor([[0.0400, 0.0608, 0.6183, 0.1105, 0.0176, 0.0448, 0.1080]])\n","[\"du coup comme emmanuel macron a exposé l'ingérence russe dans les élections de  va-t-il considérer des sanctions contre l'espagne le portugal et l'allemagne comme s'ingérant dans l'élection présidentielle française de \", 2]\n","tensor([[0.0312, 0.0256, 0.4706, 0.3958, 0.0227, 0.0214, 0.0326]])\n","['parce que macron est républicain tout pour les riches rien pour les autres le macronisme du monde trznspire un peu trop', 6]\n","tensor([[0.0524, 0.0675, 0.3812, 0.0953, 0.0291, 0.0729, 0.3016]])\n","['très bien tout mon soutien a marine', 6]\n","tensor([[0.0348, 0.0243, 0.6940, 0.1609, 0.0194, 0.0210, 0.0457]])\n","['et ils vont voter macron et ils sont contents', 2]\n","tensor([[0.0768, 0.0396, 0.6621, 0.1038, 0.0225, 0.0300, 0.0653]])\n","['le meilleur et celui qui maitrise les dossiers c est bien emmanuel macron', 2]\n","tensor([[0.1256, 0.0391, 0.5530, 0.1106, 0.0299, 0.0238, 0.1179]])\n","['emmanuel macron bien meilleur… elle a encore du boulot', 2]\n","tensor([[0.0811, 0.0366, 0.6208, 0.1253, 0.0300, 0.0273, 0.0788]])\n","[\"il y a un gouffre en macron et elle qui malgré une énorme préparation n'a pas été au niveau\", 2]\n","tensor([[0.0774, 0.0783, 0.0890, 0.0510, 0.0417, 0.0651, 0.5976]])\n","['il a joué il a loupé il pensait la déstabiliser bravo marine', 6]\n","tensor([[0.0751, 0.0320, 0.4854, 0.2874, 0.0275, 0.0255, 0.0671]])\n","[\" emmanuel macron va l'emporter c'est acté là il y a plus de débat\", 6]\n","tensor([[0.0333, 0.0289, 0.7756, 0.0800, 0.0177, 0.0223, 0.0422]])\n","['on lattendait macron avec les russes', 2]\n","tensor([[0.0312, 0.0271, 0.6114, 0.2489, 0.0189, 0.0231, 0.0394]])\n","[\"si vraiment la gauche vote pour macron c'est que tout va bien en france et que ce sont des magouilles\", 2]\n","tensor([[0.0364, 0.0304, 0.2192, 0.5938, 0.0331, 0.0290, 0.0580]])\n","['je ne voterai pas pour lepen', 3]\n","tensor([[0.0276, 0.0238, 0.3453, 0.5127, 0.0254, 0.0350, 0.0301]])\n","['la menace pour nos libertés est en marche réveillez vous stoppez macron', 3]\n","tensor([[0.0289, 0.0324, 0.6674, 0.1547, 0.0203, 0.0262, 0.0700]])\n","[\"ouai enfin macron l'a invité à versailles et lui parle tt les soir avant de se coucher\", 2]\n","tensor([[0.0328, 0.0260, 0.1567, 0.6882, 0.0343, 0.0296, 0.0325]])\n","['avec pécresse', 3]\n","tensor([[0.0309, 0.0300, 0.3182, 0.5222, 0.0239, 0.0205, 0.0543]])\n","['mélenchon un carriériste de la pire espèce', 3]\n","tensor([[0.0518, 0.0403, 0.4476, 0.3289, 0.0297, 0.0292, 0.0725]])\n","['ben cest pas compliqué aux legislatives vous votez pour mélenchon et il sera premier ministre et le presidente pourra brosser son costume', 6]\n","tensor([[0.1626, 0.0469, 0.4623, 0.1070, 0.0309, 0.0367, 0.1537]])\n","['pour contrecarrer les plans de macron ca peut être une bonne idée', 6]\n","tensor([[0.0544, 0.0273, 0.7297, 0.1034, 0.0204, 0.0194, 0.0454]])\n","[\"par contre macron qui supprime le corps diplomatique ça n'est pas du tout un déclassementles bourgeois du monde protègent encore leur classe\", 2]\n","tensor([[0.0336, 0.0289, 0.6678, 0.1653, 0.0191, 0.0252, 0.0601]])\n","[\"attention c'est peut être de la manipulation pour les élections macron fait peut-être ceci afin de récupérer un électorat se méfier\", 2]\n","tensor([[0.0510, 0.0998, 0.3171, 0.1618, 0.0385, 0.0690, 0.2628]])\n","['tant mieuxvive marine', 6]\n","tensor([[0.0488, 0.0638, 0.4348, 0.2989, 0.0248, 0.0383, 0.0905]])\n","['plus jamais macron vite marine', 6]\n","tensor([[0.1794, 0.0343, 0.5219, 0.1329, 0.0334, 0.0237, 0.0744]])\n","['oui ok pour l’article mais cela n’enlève pas le fait que l’europe impose un choix politique libéral voire même ultra libéral provoquant la disparition des services publics qui ont un monopole de fait car dits services publiques et cela représente une partie non négligeable de l’électorat de mlp n’oublions pas qu’une très grosse partie de son électorat se situe dans les campagnes et les petites villes et villages ou la disparition de ces services posent problème', 2]\n","tensor([[0.0316, 0.0287, 0.5356, 0.2865, 0.0218, 0.0239, 0.0719]])\n","['il pique à melenchon et à jadot quelle personnalité', 2]\n","tensor([[0.0455, 0.0431, 0.6643, 0.1610, 0.0203, 0.0191, 0.0467]])\n","['ah ah ah et macron va encore bien enfumer les écolos', 2]\n","tensor([[0.0705, 0.0359, 0.0894, 0.4281, 0.1670, 0.0502, 0.1590]])\n","[' eme article anti rn depuis lundi dernier pas mal comme rythme en une semaine', 6]\n","tensor([[0.0470, 0.0373, 0.6614, 0.1669, 0.0211, 0.0229, 0.0434]])\n","[\"ce gouvernement a fait plus en  ans que les politiques de gauche et de droite en  ans certains semble avoir peu de mémoire les oppositions nous ont fait avant tout du contre tout et n' ont rien proposé dans les assemblées le seul en capacité de gérer le pays c ' est emmanuel macron\", 2]\n","tensor([[0.0328, 0.0266, 0.4814, 0.3579, 0.0233, 0.0261, 0.0519]])\n","['pouvez vous nous dire qui ne vote pas macron', 6]\n","tensor([[0.0271, 0.0274, 0.3273, 0.5211, 0.0240, 0.0332, 0.0400]])\n","['en espérant que marine le pen passe ça le calmera', 3]\n","tensor([[0.0457, 0.0501, 0.1997, 0.1178, 0.0558, 0.4542, 0.0768]])\n","['il est mis en examen et sa reconduction ne serait pas un bon signe jour une éventuelle mandature de macron', 5]\n","tensor([[0.0482, 0.0699, 0.3304, 0.2201, 0.0336, 0.0430, 0.2548]])\n","[\"tous les lrem ou assimilés schlinguent puent renaudent tellement fort que quand ils passent à la tv et que je n'ai pas eu le temps de zapper je suis obligé de désinfecter ma maison entièrement\", 6]\n","tensor([[0.0295, 0.0260, 0.1276, 0.7192, 0.0327, 0.0286, 0.0364]])\n","[\"tout à fait d'accord soutien au rn et marine le pen\", 3]\n","tensor([[0.0355, 0.0310, 0.6625, 0.1774, 0.0191, 0.0229, 0.0517]])\n","['je signe je soutiens le rn j en ai marre je ne sais pas ce que macron mijote il faut que ça change', 2]\n","tensor([[0.0646, 0.0285, 0.2704, 0.5272, 0.0330, 0.0249, 0.0513]])\n","['avec le macron on est mal barrer', 3]\n","tensor([[0.0586, 0.0558, 0.2861, 0.1557, 0.0376, 0.0528, 0.3533]])\n","['vive le rn vive marine', 6]\n","tensor([[0.0628, 0.1477, 0.2107, 0.1229, 0.0358, 0.0544, 0.3657]])\n","['vive marine', 6]\n","tensor([[0.0290, 0.0260, 0.1664, 0.6906, 0.0299, 0.0277, 0.0303]])\n","['bravos à tt ceux qui on voter pour le roi surtout bien voter en juin pour les législatives pour lui empêcher tt pouvoir et tt sacage sociale après on pourra agir par référendum pour le faire destituer et marine agira en conséquence vive marine', 3]\n","tensor([[0.0281, 0.0245, 0.2006, 0.6583, 0.0261, 0.0258, 0.0365]])\n","[\"tous t'a fait d'accord vive marine le pen\", 3]\n","tensor([[0.0434, 0.0298, 0.2135, 0.5631, 0.0326, 0.0305, 0.0871]])\n","[\"tout à fait d'accord je soutien au rn avec vous\", 3]\n","tensor([[0.0893, 0.0939, 0.0680, 0.1131, 0.0513, 0.0797, 0.5048]])\n","[\"entièrement d'accord avec vous marine\", 6]\n","tensor([[0.0988, 0.0580, 0.0536, 0.0832, 0.4147, 0.0752, 0.2164]])\n","[' et rn', 4]\n","tensor([[0.0389, 0.0342, 0.6774, 0.1497, 0.0212, 0.0185, 0.0601]])\n","['je signe et je soutien le rn j en peu plus macron nous mijoté quelque chose et ça ne sent pas bon il faut que ça change', 2]\n","tensor([[0.0628, 0.1477, 0.2107, 0.1229, 0.0358, 0.0544, 0.3657]])\n","['vive marine', 6]\n","tensor([[0.0310, 0.0285, 0.1528, 0.6762, 0.0337, 0.0291, 0.0487]])\n","['pour sa il fallait voter marine', 3]\n","tensor([[0.0476, 0.0395, 0.1487, 0.5834, 0.0505, 0.0388, 0.0914]])\n","['voté pour marine oui', 3]\n","tensor([[0.0950, 0.0725, 0.0888, 0.0528, 0.0456, 0.0590, 0.5863]])\n","['et une dernière chose vive le rn', 6]\n","tensor([[0.0444, 0.0324, 0.6647, 0.1607, 0.0191, 0.0215, 0.0571]])\n","['si je me rappelle bien il y a pas longtemps on avais l occasion de faire barrage pour ce genre de situation résultat macron est sorti vainqueur des urnes et cette situation continue à titre d informations', 2]\n","tensor([[0.0831, 0.0472, 0.1769, 0.2975, 0.0462, 0.0436, 0.3055]])\n","['je signe et je soutiens le rn', 6]\n","tensor([[0.0628, 0.1477, 0.2107, 0.1229, 0.0358, 0.0544, 0.3657]])\n","['vive marine', 6]\n","tensor([[0.0546, 0.0479, 0.1355, 0.4260, 0.0426, 0.0619, 0.2313]])\n","['marine le pen', 6]\n","tensor([[0.0295, 0.0271, 0.5603, 0.3015, 0.0207, 0.0267, 0.0342]])\n","['expulsez déjà marine lepen histoire que la vraie droite puisse gagner', 2]\n","tensor([[0.0699, 0.0479, 0.4665, 0.1849, 0.0276, 0.0346, 0.1686]])\n","['eh ouibien d accord mais le rn s est encore ramasse aux elections', 6]\n","tensor([[0.0397, 0.0633, 0.2875, 0.0616, 0.0302, 0.3913, 0.1263]])\n","['oui au moins marine les aurait mit dans un  avion', 5]\n","tensor([[0.0628, 0.1477, 0.2107, 0.1229, 0.0358, 0.0544, 0.3657]])\n","['vive marine', 6]\n","tensor([[0.0619, 0.0346, 0.2982, 0.4051, 0.0357, 0.0267, 0.1377]])\n","['je signe et suis pour le rn il faut que tout ça cesse', 6]\n","tensor([[0.0538, 0.0355, 0.1266, 0.5977, 0.0631, 0.0305, 0.0926]])\n","['voter rn massivement', 3]\n","tensor([[0.0476, 0.0395, 0.1487, 0.5834, 0.0505, 0.0388, 0.0914]])\n","['voté pour marine oui', 3]\n","tensor([[0.0347, 0.0266, 0.7147, 0.1467, 0.0190, 0.0210, 0.0374]])\n","['ils ont votés macron et bien maintenant qu ils subissent dommage pour les autres', 2]\n","tensor([[0.1266, 0.0715, 0.1285, 0.1016, 0.0555, 0.0407, 0.4756]])\n","['je ne signerai plus aucune pétition venant du rn', 6]\n","tensor([[0.0255, 0.0323, 0.6938, 0.1113, 0.0193, 0.0778, 0.0399]])\n","['ha ouais du grand n importe quoi pour qu’ils reviennent en france par une autre porte et faire des attentas vous êtes nuls les rn mieux vaux les garder en prisons au moins on se sent plus en sécurité', 2]\n","tensor([[0.0276, 0.0301, 0.7956, 0.0666, 0.0171, 0.0237, 0.0392]])\n","['dehors tous ces envahisseurs ainsi que ce gouvernement incapable vendu corrompus vive le rn réveillez vous français', 2]\n","tensor([[0.0664, 0.0383, 0.3583, 0.2822, 0.0354, 0.0322, 0.1871]])\n","[\"tout à fait d'accord soutien au rn\", 6]\n","tensor([[0.0273, 0.0249, 0.0893, 0.7709, 0.0325, 0.0271, 0.0281]])\n","[\"comme dit jordan bardella le burkini n'a rien à faire dans nos piscines françaises dans les pays du maghreb nous français nous devons nous soumettre à leurs coutumes donc en france pour eux c est la même chose ils doivent s adapter aux coutumes françaises\", 3]\n","tensor([[0.0265, 0.0237, 0.1275, 0.7389, 0.0300, 0.0251, 0.0283]])\n","[\"et puis quoi encore elle es belle la france pfffff je suis contre le burkini n'a rien à faire dans les piscines en france je vois que tout tourne autour d'eux mais rien pour les français une honte vive marine\", 3]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-26228313c0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0;31m#writer = csv.writer(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                       \u001b[0;31m#writer.writerow([x, test5(x)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-a79bdf64f7dd>\u001b[0m in \u001b[0;36mtest5\u001b[0;34m(comment)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprediction_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         )\n\u001b[1;32m   1216\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import csv\n","d=pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/filter/data1_filté.csv\")\n","with open(\"/content/drive/MyDrive/Colab_Notebooks/classification_camembert/resultat49.csv\", 'a', encoding='UTF8') as f:\n","                    #writer = csv.writer(f)\n","                    for x in d['comment']:\n","                      print([x, test5(x)])\n","                      #writer.writerow([x, test5(x)])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1653577138906,"user":{"displayName":"aymen sahnoun","userId":"11561670817046976184"},"user_tz":-120},"id":"eOHVvWEC4Eng","outputId":"386a16f1-d05a-4c8d-9b5b-6ad35efa3e5b"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-1e47d390-cea4-474b-8581-1bbb88e41d0e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2000</th>\n","      <td>laïcisation voile sécularité séparation laïcis...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2001</th>\n","      <td>musulman sécularité séparatisme religieux loi ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2002</th>\n","      <td>sécularité sécularité laïcité laïque séparatis...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2003</th>\n","      <td>religieux religion musulman séparation laïcité...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2004</th>\n","      <td>séparation religion islamophobie islamophobie ...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2495</th>\n","      <td>sécularité séparation voile religieux signe re...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2496</th>\n","      <td>musulman séparatisme religieux religion laïcis...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2497</th>\n","      <td>séparatisme religieux islamophobie voile loi d...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2498</th>\n","      <td>signe religieux laïcité islamophobie sécularit...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2499</th>\n","      <td>loi de 1905 islamophobie séparation séparatism...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e47d390-cea4-474b-8581-1bbb88e41d0e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1e47d390-cea4-474b-8581-1bbb88e41d0e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1e47d390-cea4-474b-8581-1bbb88e41d0e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                   text  label\n","2000  laïcisation voile sécularité séparation laïcis...      3\n","2001  musulman sécularité séparatisme religieux loi ...      3\n","2002  sécularité sécularité laïcité laïque séparatis...      3\n","2003  religieux religion musulman séparation laïcité...      3\n","2004  séparation religion islamophobie islamophobie ...      3\n","...                                                 ...    ...\n","2495  sécularité séparation voile religieux signe re...      3\n","2496  musulman séparatisme religieux religion laïcis...      3\n","2497  séparatisme religieux islamophobie voile loi d...      3\n","2498  signe religieux laïcité islamophobie sécularit...      3\n","2499  loi de 1905 islamophobie séparation séparatism...      3\n","\n","[500 rows x 2 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["cle=cles[cles['label']==3]\n","cle"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"Camembert-base-maram.ipynb","provenance":[{"file_id":"1Ltu3eWg2HDBKWICsgDXU6QwNW1O6Rr7H","timestamp":1653153314765}],"mount_file_id":"1Yb3bjT_pMXOYILxVFqqgEkeyG2U0xy7p","authorship_tag":"ABX9TyMAxuidS1V/+YSnelO5o+my"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"223eb1b7dda74749b5bdc60af8ec96ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c6a587366e74f1d90c26d61236ac444","IPY_MODEL_29db9ba85a8b43f4b36454ecc734346a","IPY_MODEL_85568fcf2861407fb42ff5e5701abcf8"],"layout":"IPY_MODEL_d72cb85a97d74aabbd308395c82da5a2"}},"4c6a587366e74f1d90c26d61236ac444":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_284e5e4d9a3d4595ab39fd0008e907fa","placeholder":"​","style":"IPY_MODEL_d7ee581af2724b39acb895af9dd0aa29","value":"Downloading: 100%"}},"29db9ba85a8b43f4b36454ecc734346a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e65b755454c342c2acb61fd815d06e16","max":508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_080c9813f8f443dbba50328e273e3384","value":508}},"85568fcf2861407fb42ff5e5701abcf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a11a20c92124f10868f59886ff6f284","placeholder":"​","style":"IPY_MODEL_dea12f44f1484369b94d88a4522aa6a6","value":" 508/508 [00:00&lt;00:00, 3.50kB/s]"}},"d72cb85a97d74aabbd308395c82da5a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"284e5e4d9a3d4595ab39fd0008e907fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7ee581af2724b39acb895af9dd0aa29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e65b755454c342c2acb61fd815d06e16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"080c9813f8f443dbba50328e273e3384":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a11a20c92124f10868f59886ff6f284":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dea12f44f1484369b94d88a4522aa6a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9cf395df7f704203b3f86169b15d739b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78e98d8399b04b3da1323bf134fbeebd","IPY_MODEL_96f51e8b8924435999a349e524646263","IPY_MODEL_6b4fe3290d734c0aba849fbd1975d9a8"],"layout":"IPY_MODEL_ade3bfd5938745ac964b68549c3e4925"}},"78e98d8399b04b3da1323bf134fbeebd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f183c4711a984f01b290c2ac710c3e2d","placeholder":"​","style":"IPY_MODEL_68d8a3062e714bcdbda0dd2e3746530e","value":"Downloading: 100%"}},"96f51e8b8924435999a349e524646263":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73b4093432fe4b0c867ce5738b9b92e9","max":810912,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3db5a3c366545e5b03f9666266df9e9","value":810912}},"6b4fe3290d734c0aba849fbd1975d9a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62dff3a3c4474802b56ecc8a05043f43","placeholder":"​","style":"IPY_MODEL_1807b4b503b34cc4b2c8b43a87109475","value":" 792k/792k [00:00&lt;00:00, 1.06MB/s]"}},"ade3bfd5938745ac964b68549c3e4925":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f183c4711a984f01b290c2ac710c3e2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68d8a3062e714bcdbda0dd2e3746530e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73b4093432fe4b0c867ce5738b9b92e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3db5a3c366545e5b03f9666266df9e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62dff3a3c4474802b56ecc8a05043f43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1807b4b503b34cc4b2c8b43a87109475":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4f2cfe921904402b37218e194927f46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c18373f88ea44976a00f26a223e5dc40","IPY_MODEL_9b01c14604ea48c393154e22a5993b42","IPY_MODEL_4961ab62da3f456c8e56372840622104"],"layout":"IPY_MODEL_7a370b9bc5c0497b96778f69265dd25f"}},"c18373f88ea44976a00f26a223e5dc40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5af59a76363743118b9c04a94e93743b","placeholder":"​","style":"IPY_MODEL_cc08c7d7e7e845ba9f4dde8d63536d03","value":"Downloading: 100%"}},"9b01c14604ea48c393154e22a5993b42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ee1c407544f452cb754dd3c72116ad5","max":1395301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67b1643e2d4c4e58a8fff7f92ff64d19","value":1395301}},"4961ab62da3f456c8e56372840622104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2636c88b3b9e4a9da068dc6193f13f54","placeholder":"​","style":"IPY_MODEL_8acd11b625e542b5ad4977705550e7a9","value":" 1.33M/1.33M [00:00&lt;00:00, 1.48MB/s]"}},"7a370b9bc5c0497b96778f69265dd25f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5af59a76363743118b9c04a94e93743b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc08c7d7e7e845ba9f4dde8d63536d03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ee1c407544f452cb754dd3c72116ad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67b1643e2d4c4e58a8fff7f92ff64d19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2636c88b3b9e4a9da068dc6193f13f54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8acd11b625e542b5ad4977705550e7a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}